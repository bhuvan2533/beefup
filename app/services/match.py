from sentence_transformers import SentenceTransformer, util
import nltk
nltk.download('stopwords', quiet=True)
import re
from nltk.corpus import stopwords


profile_text = resume_string = "Praveen J\nAI/ML Engineer\nWith 4.5+ years of experience as an AI/ML Engineer specializing in NLP, I focus on building intelligent document understanding pipelines and extracting structured data from complex documents using tools like transformers, spaCy, and NLTK. I have hands-on experience with layout-aware models and knowledge graph construction using Neo4j. A strong problem-solver and collaborative team player, I'm driven by a passion for continuous learning and innovation in document processing and machine learning applications.\n\nKey Technologies:\n\tPackages - Numpy, Pandas, pytorch, sentence_transformers, Scikit-learn, Seaborn, langchain, llamaindex, Nltk, XGBoost, MLflow, Keras.\n\tTools - Jupyter, VSCode, PyCharm, Neptune\n\tCloud technologies - Databricks, AWS\n\tDatabase - MySql, Sqlite, Postgresql, mongodb, neo4j\n\tBackend Technologies - Django, Django Rest Framework, fastapi\n\tVisualisation Technologies - Tableau, PowerBI, Mapbox\n\nProject 1: Sentistock\n\tSkills: Python, NLP, Machine Learning, Data Analysis, Sentimental Analysis, Time series Prediction\n\tDescription: Advanced NLP techniques were used to analyze tweets related to companies and correlate sentiments with stock price movements. Models like ARIMA, LSTM, and Regression were applied for trend prediction.\n\tResponsibilities: Twitter API integration, data pipelines, sentiment analysis, LSTM & ARIMA modeling.\n\nProject 2: MetaSecPDF\n\tSkills: Kali Linux, Metasploit, Python, PDF analysis, Machine learning\n\tDescription: Classified PDFs as malicious or safe using ML. Simulated malware attacks, used classification models with Scikit-learn and TensorFlow.\n\tResponsibilities: Malware simulation, feature extraction, real-world validation.\n\nProject 3: Generating Text in Shakespearean Style\n\tSkills: Keras, Tensorflow, Pandas, numpy, scipy, gutenberg, Text generation\n\tDescription: Trained char-level RNN on Shakespeare's works for text generation.\n\tResponsibilities: Data harvesting, RNN training, variability control, activation visualization.\n\nProject 4: Youtube AI Categorization\n\tSkills: Python, NLP, LLM, Langchain, Mongodb\n\tDescription: Used LLMs to categorize YouTube videos by transcript.\n\tResponsibilities: ETL pipeline, LLM prompting, data transformation, categorization.\n\nProject 5: FAQ Chatbot\n\tSkills: Llama index, LLM, NLP, Python, FastAPI, Document Retrieval\n\tDescription: Built chatbot for FAQs using LLM and document upload.\n\tResponsibilities: FastAPI integration, document retrieval, chatbot response accuracy.\n\nProject 6: Computer Vision Projects with Neptune.ai\n\tSkills: Neptune.ai, Computer vision, Pytorch Lightning, Jupyter Notebook, Model tracking, Data logging\n\tDescription: Used Neptune.ai for CV model tracking and experimentation.\n\tResponsibilities: Model monitoring, training metrics, experiment dashboards.\n\nProject 7: Job Recommendation Engine\n\tSkills: Cross encoder score, Content similarity, FastAPI, Docker, Langchain, Redis, PostgreSQL\n\tDescription: Built job recommendation engine using hybrid NLP scoring methods.\n\tResponsibilities: Resume/JD parsing, LLM prompting, skill matching.\n\nProject 8: Tracking Experimentation\n\tSkills: Neptune.ai, XGBoost, ML, Data visualization\n\tDescription: Logged and visualized ML experiments.\n\tResponsibilities: XGBoost + Neptune integration, training visualizations.\n\nProject 9: CrowdFlow\n\tSkills: Arduino, Microcontrollers, C++, Python, ML\n\tDescription: COVID crowd monitoring system using Arduino and ML.\n\tResponsibilities: System implementation, prediction model integration.\n\nProject 10: SurfaceAI\n\tSkills: Python, Deep Learning, CNN, Image classification\n\tDescription: Detected surface defects using CNN and transfer learning.\n\tResponsibilities: VGGNet fine-tuning, data augmentation, evaluation."

# profile_text ="Praveen J\nAI/ML Engineer - NLP/ML Engineering, Document Understanding, Knowledge Graphs\nWith 4.5+ years of experience as an AI/ML Engineer specializing in NLP, document understanding, and knowledge graph construction, I focus on building intelligent document understanding pipelines that extract structured data from complex documents such as RFQs. I have hands-on experience using layout-aware models (e.g., LayoutLM, Donut) to segment and parse documents, and I specialize in multilingual entity recognition (English and German) and relation extraction. I have worked extensively with tools like transformers, spaCy, and NLTK, and built scalable systems for extracting meaningful information from unstructured text. I am proficient in using Neo4j for constructing and maintaining knowledge graphs to represent semantic relationships. A strong problem-solver and collaborative team player, I'm driven by a passion for continuous learning, innovation in document processing, and integrating these systems into large-scale applications like LLMs for downstream tasks.\n\nKey Technologies:\n\tPackages - Numpy, Pandas, pytorch, sentence_transformers, Scikit-learn, Seaborn, langchain, llamaindex, Nltk, XGBoost, MLflow, Keras, transformers, spaCy\n\tTools - Jupyter, VSCode, PyCharm, Neptune\n\tCloud technologies - Databricks, AWS\n\tDatabase - MySql, Sqlite, Postgresql, mongodb, neo4j, RDF\n\tBackend Technologies - Django, Django Rest Framework, fastapi\n\tVisualisation Technologies - Tableau, PowerBI, Mapbox\n\tKnowledge Graphs - Neo4j, RDF, SPARQL\n\tLayout-Aware Models - LayoutLM, Donut, DocLayout-YOLO\n\nProject 1: Sentistock\n\tSkills: Python, NLP, Machine Learning, Data Analysis, Sentimental Analysis, Time series Prediction\n\tDescription: Leveraged NLP techniques to analyze multilingual tweets related to companies and correlate sentiments with stock price movements. The system incorporated time-series forecasting using ARIMA and LSTM models to predict stock trends. The project utilized real-time Twitter API data to analyze sentiment and its correlation with market activities.\n\tResponsibilities: Integrated Twitter API for sentiment extraction, developed data pipelines, performed sentiment analysis, built prediction models using LSTM and ARIMA.\n\nProject 2: MetaSecPDF\n\tSkills: Kali Linux, Metasploit, Python, PDF analysis, Machine learning\n\tDescription: Built a system to classify PDFs as malicious or non-malicious using machine learning algorithms. Leveraged feature extraction techniques and applied classification models like Scikit-learn and TensorFlow to detect potential threats in PDF documents.\n\tResponsibilities: Created malware simulation in PDFs, developed feature extraction methods, and applied machine learning classification models to detect malicious PDFs.\n\nProject 3: Generating Text in Shakespearean Style\n\tSkills: Keras, Tensorflow, Pandas, numpy, scipy, gutenberg, Text generation\n\tDescription: Trained a character-level RNN on Shakespeareâ€™s works to generate text in his style, showcasing deep learning techniques for NLP and creative applications.\n\tResponsibilities: Harvested data from public domain books, trained an RNN on the collected works, fine-tuned model for text generation with probability distribution.\n\nProject 4: YouTube AI Categorization\n\tSkills: Python, NLP, LLM, Langchain, Mongodb\n\tDescription: Built a data pipeline to scrape YouTube channels and categorize videos using NLP techniques and LLMs. Focused on transforming unstructured video data into structured formats like JSON and Markdown for downstream analysis.\n\tResponsibilities: Developed ETL pipeline for YouTube data, used LLM for content categorization based on video transcripts, transformed unstructured data into structured formats.\n\nProject 5: FAQ Chatbot\n\tSkills: Llama index, LLM, NLP, Python, FastAPI, Document Retrieval\n\tDescription: Developed an intelligent FAQ chatbot capable of handling document retrieval and answering user queries based on uploaded documents (CSV, PDF). Used LlamaIndex and FastAPI for seamless integration.\n\tResponsibilities: Created FastAPI application, implemented document retrieval, fine-tuned LLM for accurate FAQ-based queries.\n\nProject 6: Computer Vision Projects with Neptune.ai\n\tSkills: Neptune.ai, Computer vision, Pytorch Lightning, Jupyter Notebook, Model tracking, Data logging\n\tDescription: Used Neptune.ai to monitor and track computer vision models, including training logs, performance metrics, and confusion matrices to ensure model reproducibility and stability.\n\tResponsibilities: Implemented Neptune.ai for live monitoring, logging training metadata, and visualizing model behavior during training.\n\nProject 7: Job Recommendation Engine\n\tSkills: Cross encoder score, Content similarity, FastAPI, Docker, Langchain, Redis, PostgreSQL\n\tDescription: Built a job recommendation engine using NLP techniques and LLMs to match resumes and job descriptions. The hybrid scoring system involved cross-encoders and content similarity algorithms for precise job recommendations.\n\tResponsibilities: Developed job recommendation algorithm, integrated FastAPI for job matching, implemented Redis and PostgreSQL for efficient storage.\n\nProject 8: Tracking Experimentation\n\tSkills: Neptune.ai, XGBoost, ML, Data visualization\n\tDescription: Integrated Neptune.ai for logging and tracking machine learning experiments. Focused on XGBoost models and visualizing performance metrics to enhance model accuracy and reliability.\n\tResponsibilities: Logged XGBoost model metadata, visualized feature importances, and compared different model runs using Neptune.ai.\n\nProject 9: CrowdFlow\n\tSkills: Arduino, Microcontrollers, C++, Python, Machine Learning\n\tDescription: Developed a crowd monitoring system using Arduino microcontrollers and ML algorithms to track people flow and ensure effective social distancing during the COVID-19 pandemic.\n\tResponsibilities: Designed real-time crowd monitoring system, integrated machine learning models for traffic prediction, ensured safe environment for businesses.\n\nProject 10: SurfaceAI\n\tSkills: Python, Deep Learning, CNN, Image classification\n\tDescription: Built a surface defect detection system using CNN and transfer learning (VGGNet), achieving a high accuracy of 97%. The model utilized K-fold cross-validation to ensure robustness and reduce overfitting.\n\tResponsibilities: Applied CNN and transfer learning techniques, employed K-fold cross-validation for model evaluation, optimized model performance for surface defect classification."

jd_text =  "NLP Engineer / Machine Learning Engineer - Document Understanding & Knowledge Graphs experience: 4-5 years Overview Wre looking for a hands-on NLP/ML engineer to lead the development of an intelligent document understanding pipeline for extracting structured data from complex, unstructured RFQ documents (40â€“100+ pages, in German and English). You will be responsible for building scalable systems that combine document parsing, layout analysis, entity extraction, and knowledge graph construction â€” ultimately feeding downstream (e.g. Analytics and LLM applications.) Key Responsibilities Design and implement document hierarchy and section segmentation pipelines using layout-aware models (e.g., DocLayout-YOLO, LayoutLM, Donut). Build multilingual entity recognition and relation extraction systems across both English and German texts. Use tools like NLTK, transformers, and spaCy to develop custom tokenization, parsing, and information extraction logic. Construct and maintain knowledge graphs representing semantic relationships between extracted elements using graph data structures and graph databases (e.g. Neo4j) Integrate outputs into structured LLM-friendly formats (e.g., JSON, Mark Down) for downstream extraction of building material elements. Collaborate with product and domain experts to align on information schema, ontology, and validation methods. What Weâ€™re Looking For Strong experience in NLP, document understanding, and information extraction from unstructured/multilingual documents. Proficiency in Python, with experience using libraries such as transformers, spaCy, and NLTK. Hands-on experience with layout-aware models like DocLayout-YOLO, LayoutLM, Donut, or similar. Familiarity with knowledge graphs and graph databases such as Neo4j, RDF Understanding of prompt engineering and how to structure data for LLM-based tasks. Bonus: Experience with technical construction documents, RFQs, or engineering-based document formats."


def extract_hotwords(text):
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    words = clean_text.split()
    stop_words = set(stopwords.words('english')) 
    hotwords = [word for word in words if word.lower() not in stop_words]

    return hotwords


def calculate_similarity(profile_content, jd_content):
    model = SentenceTransformer('all-MiniLM-L6-v2') 

    # Extract hotwords from both texts
    profile_hotwords = extract_hotwords(profile_content)
    jd_hotwords = extract_hotwords(jd_content)
    profile_text = " ".join(profile_hotwords)
    jd_text = " ".join(jd_hotwords)


    embeddings = model.encode([profile_text, jd_text], convert_to_tensor=True)

    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()

    match_percentage = round(similarity_score * 100, 2)
    
    return match_percentage


print(calculate_similarity(profile_text, jd_text))