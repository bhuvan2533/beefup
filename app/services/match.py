from sentence_transformers import SentenceTransformer, util
import nltk
nltk.download('stopwords', quiet=True)
import re
from nltk.corpus import stopwords

#only for testing purpose
# profile_text = resume_string = "Praveen J\nAI/ML Engineer\nWith 4.5+ years of experience as an AI/ML Engineer specializing in NLP, I focus on building intelligent document understanding pipelines and extracting structured data from complex documents using tools like transformers, spaCy, and NLTK. I have hands-on experience with layout-aware models and knowledge graph construction using Neo4j. A strong problem-solver and collaborative team player, I'm driven by a passion for continuous learning and innovation in document processing and machine learning applications.\n\nKey Technologies:\n\tPackages - Numpy, Pandas, pytorch, sentence_transformers, Scikit-learn, Seaborn, langchain, llamaindex, Nltk, XGBoost, MLflow, Keras.\n\tTools - Jupyter, VSCode, PyCharm, Neptune\n\tCloud technologies - Databricks, AWS\n\tDatabase - MySql, Sqlite, Postgresql, mongodb, neo4j\n\tBackend Technologies - Django, Django Rest Framework, fastapi\n\tVisualisation Technologies - Tableau, PowerBI, Mapbox\n\nProject 1: Sentistock\n\tSkills: Python, NLP, Machine Learning, Data Analysis, Sentimental Analysis, Time series Prediction\n\tDescription: Advanced NLP techniques were used to analyze tweets related to companies and correlate sentiments with stock price movements. Models like ARIMA, LSTM, and Regression were applied for trend prediction.\n\tResponsibilities: Twitter API integration, data pipelines, sentiment analysis, LSTM & ARIMA modeling.\n\nProject 2: MetaSecPDF\n\tSkills: Kali Linux, Metasploit, Python, PDF analysis, Machine learning\n\tDescription: Classified PDFs as malicious or safe using ML. Simulated malware attacks, used classification models with Scikit-learn and TensorFlow.\n\tResponsibilities: Malware simulation, feature extraction, real-world validation.\n\nProject 3: Generating Text in Shakespearean Style\n\tSkills: Keras, Tensorflow, Pandas, numpy, scipy, gutenberg, Text generation\n\tDescription: Trained char-level RNN on Shakespeare's works for text generation.\n\tResponsibilities: Data harvesting, RNN training, variability control, activation visualization.\n\nProject 4: Youtube AI Categorization\n\tSkills: Python, NLP, LLM, Langchain, Mongodb\n\tDescription: Used LLMs to categorize YouTube videos by transcript.\n\tResponsibilities: ETL pipeline, LLM prompting, data transformation, categorization.\n\nProject 5: FAQ Chatbot\n\tSkills: Llama index, LLM, NLP, Python, FastAPI, Document Retrieval\n\tDescription: Built chatbot for FAQs using LLM and document upload.\n\tResponsibilities: FastAPI integration, document retrieval, chatbot response accuracy.\n\nProject 6: Computer Vision Projects with Neptune.ai\n\tSkills: Neptune.ai, Computer vision, Pytorch Lightning, Jupyter Notebook, Model tracking, Data logging\n\tDescription: Used Neptune.ai for CV model tracking and experimentation.\n\tResponsibilities: Model monitoring, training metrics, experiment dashboards.\n\nProject 7: Job Recommendation Engine\n\tSkills: Cross encoder score, Content similarity, FastAPI, Docker, Langchain, Redis, PostgreSQL\n\tDescription: Built job recommendation engine using hybrid NLP scoring methods.\n\tResponsibilities: Resume/JD parsing, LLM prompting, skill matching.\n\nProject 8: Tracking Experimentation\n\tSkills: Neptune.ai, XGBoost, ML, Data visualization\n\tDescription: Logged and visualized ML experiments.\n\tResponsibilities: XGBoost + Neptune integration, training visualizations.\n\nProject 9: CrowdFlow\n\tSkills: Arduino, Microcontrollers, C++, Python, ML\n\tDescription: COVID crowd monitoring system using Arduino and ML.\n\tResponsibilities: System implementation, prediction model integration.\n\nProject 10: SurfaceAI\n\tSkills: Python, Deep Learning, CNN, Image classification\n\tDescription: Detected surface defects using CNN and transfer learning.\n\tResponsibilities: VGGNet fine-tuning, data augmentation, evaluation."
# enhanced_text ="Praveen J\nAI/ML Engineer - NLP/ML Engineering, Document Understanding, Knowledge Graphs\nWith 4.5+ years of experience as an AI/ML Engineer specializing in NLP, document understanding, and knowledge graph construction, I focus on building intelligent document understanding pipelines that extract structured data from complex documents such as RFQs. I have hands-on experience using layout-aware models (e.g., LayoutLM, Donut) to segment and parse documents, and I specialize in multilingual entity recognition (English and German) and relation extraction. I have worked extensively with tools like transformers, spaCy, and NLTK, and built scalable systems for extracting meaningful information from unstructured text. I am proficient in using Neo4j for constructing and maintaining knowledge graphs to represent semantic relationships. A strong problem-solver and collaborative team player, I'm driven by a passion for continuous learning, innovation in document processing, and integrating these systems into large-scale applications like LLMs for downstream tasks.\n\nKey Technologies:\n\tPackages - Numpy, Pandas, pytorch, sentence_transformers, Scikit-learn, Seaborn, langchain, llamaindex, Nltk, XGBoost, MLflow, Keras, transformers, spaCy\n\tTools - Jupyter, VSCode, PyCharm, Neptune\n\tCloud technologies - Databricks, AWS\n\tDatabase - MySql, Sqlite, Postgresql, mongodb, neo4j, RDF\n\tBackend Technologies - Django, Django Rest Framework, fastapi\n\tVisualisation Technologies - Tableau, PowerBI, Mapbox\n\tKnowledge Graphs - Neo4j, RDF, SPARQL\n\tLayout-Aware Models - LayoutLM, Donut, DocLayout-YOLO\n\nProject 1: Sentistock\n\tSkills: Python, NLP, Machine Learning, Data Analysis, Sentimental Analysis, Time series Prediction\n\tDescription: Leveraged NLP techniques to analyze multilingual tweets related to companies and correlate sentiments with stock price movements. The system incorporated time-series forecasting using ARIMA and LSTM models to predict stock trends. The project utilized real-time Twitter API data to analyze sentiment and its correlation with market activities.\n\tResponsibilities: Integrated Twitter API for sentiment extraction, developed data pipelines, performed sentiment analysis, built prediction models using LSTM and ARIMA.\n\nProject 2: MetaSecPDF\n\tSkills: Kali Linux, Metasploit, Python, PDF analysis, Machine learning\n\tDescription: Built a system to classify PDFs as malicious or non-malicious using machine learning algorithms. Leveraged feature extraction techniques and applied classification models like Scikit-learn and TensorFlow to detect potential threats in PDF documents.\n\tResponsibilities: Created malware simulation in PDFs, developed feature extraction methods, and applied machine learning classification models to detect malicious PDFs.\n\nProject 3: Generating Text in Shakespearean Style\n\tSkills: Keras, Tensorflow, Pandas, numpy, scipy, gutenberg, Text generation\n\tDescription: Trained a character-level RNN on Shakespeare’s works to generate text in his style, showcasing deep learning techniques for NLP and creative applications.\n\tResponsibilities: Harvested data from public domain books, trained an RNN on the collected works, fine-tuned model for text generation with probability distribution.\n\nProject 4: YouTube AI Categorization\n\tSkills: Python, NLP, LLM, Langchain, Mongodb\n\tDescription: Built a data pipeline to scrape YouTube channels and categorize videos using NLP techniques and LLMs. Focused on transforming unstructured video data into structured formats like JSON and Markdown for downstream analysis.\n\tResponsibilities: Developed ETL pipeline for YouTube data, used LLM for content categorization based on video transcripts, transformed unstructured data into structured formats.\n\nProject 5: FAQ Chatbot\n\tSkills: Llama index, LLM, NLP, Python, FastAPI, Document Retrieval\n\tDescription: Developed an intelligent FAQ chatbot capable of handling document retrieval and answering user queries based on uploaded documents (CSV, PDF). Used LlamaIndex and FastAPI for seamless integration.\n\tResponsibilities: Created FastAPI application, implemented document retrieval, fine-tuned LLM for accurate FAQ-based queries.\n\nProject 6: Computer Vision Projects with Neptune.ai\n\tSkills: Neptune.ai, Computer vision, Pytorch Lightning, Jupyter Notebook, Model tracking, Data logging\n\tDescription: Used Neptune.ai to monitor and track computer vision models, including training logs, performance metrics, and confusion matrices to ensure model reproducibility and stability.\n\tResponsibilities: Implemented Neptune.ai for live monitoring, logging training metadata, and visualizing model behavior during training.\n\nProject 7: Job Recommendation Engine\n\tSkills: Cross encoder score, Content similarity, FastAPI, Docker, Langchain, Redis, PostgreSQL\n\tDescription: Built a job recommendation engine using NLP techniques and LLMs to match resumes and job descriptions. The hybrid scoring system involved cross-encoders and content similarity algorithms for precise job recommendations.\n\tResponsibilities: Developed job recommendation algorithm, integrated FastAPI for job matching, implemented Redis and PostgreSQL for efficient storage.\n\nProject 8: Tracking Experimentation\n\tSkills: Neptune.ai, XGBoost, ML, Data visualization\n\tDescription: Integrated Neptune.ai for logging and tracking machine learning experiments. Focused on XGBoost models and visualizing performance metrics to enhance model accuracy and reliability.\n\tResponsibilities: Logged XGBoost model metadata, visualized feature importances, and compared different model runs using Neptune.ai.\n\nProject 9: CrowdFlow\n\tSkills: Arduino, Microcontrollers, C++, Python, Machine Learning\n\tDescription: Developed a crowd monitoring system using Arduino microcontrollers and ML algorithms to track people flow and ensure effective social distancing during the COVID-19 pandemic.\n\tResponsibilities: Designed real-time crowd monitoring system, integrated machine learning models for traffic prediction, ensured safe environment for businesses.\n\nProject 10: SurfaceAI\n\tSkills: Python, Deep Learning, CNN, Image classification\n\tDescription: Built a surface defect detection system using CNN and transfer learning (VGGNet), achieving a high accuracy of 97%. The model utilized K-fold cross-validation to ensure robustness and reduce overfitting.\n\tResponsibilities: Applied CNN and transfer learning techniques, employed K-fold cross-validation for model evaluation, optimized model performance for surface defect classification."
# jd_text =  "NLP Engineer / Machine Learning Engineer - Document Understanding & Knowledge Graphs experience: 4-5 years Overview Wre looking for a hands-on NLP/ML engineer to lead the development of an intelligent document understanding pipeline for extracting structured data from complex, unstructured RFQ documents (40–100+ pages, in German and English). You will be responsible for building scalable systems that combine document parsing, layout analysis, entity extraction, and knowledge graph construction — ultimately feeding downstream (e.g. Analytics and LLM applications.) Key Responsibilities Design and implement document hierarchy and section segmentation pipelines using layout-aware models (e.g., DocLayout-YOLO, LayoutLM, Donut). Build multilingual entity recognition and relation extraction systems across both English and German texts. Use tools like NLTK, transformers, and spaCy to develop custom tokenization, parsing, and information extraction logic. Construct and maintain knowledge graphs representing semantic relationships between extracted elements using graph data structures and graph databases (e.g. Neo4j) Integrate outputs into structured LLM-friendly formats (e.g., JSON, Mark Down) for downstream extraction of building material elements. Collaborate with product and domain experts to align on information schema, ontology, and validation methods. What We’re Looking For Strong experience in NLP, document understanding, and information extraction from unstructured/multilingual documents. Proficiency in Python, with experience using libraries such as transformers, spaCy, and NLTK. Hands-on experience with layout-aware models like DocLayout-YOLO, LayoutLM, Donut, or similar. Familiarity with knowledge graphs and graph databases such as Neo4j, RDF Understanding of prompt engineering and how to structure data for LLM-based tasks. Bonus: Experience with technical construction documents, RFQs, or engineering-based document formats."


jd_text = "Python developer - 2 to 3 years experience \n\nLocation: Bangalore \n\nStrong proficiency in Django and Django REST Framework (DRF). \n\nExperience working with third-party APIs, webhooks, and system integrations. \n\nProficiency in Python, SQL databases (e.g., PostgreSQL, MySQL). \n\nHands-on experience with asynchronous task queues (Celery, Redis, etc.). \n\nFamiliarity with authentication mechanisms (OAuth, SSO) and security best practices. \n\nExperience developing with React.js s a bonus. \n\nStrong  problem-solving  skills  and  the  ability  to  adapt  to  new  technologies  and  requirements \n\nFunctional: \n\nquickly. \n\nAbility to detect and debug problems \n\nExcellent communication skills \n\nAbility to work independently or within a team. \n\n\f"
profile_text = "Name: Sourabh C\nBackend Developer\nWith 2.5 years of experience, I am a driven and adaptable software developer with a passion for programming and tackling challenges. I bring strong analytical and problem-solving skills, a love for learning, and a commitment to delivering impactful solutions. Known for my strong work ethic and continuous growth mindset, I strive to exceed expectations through high-quality, efficient development.\n\nKey Technologies:\nBackend Technologies - Django, FastAPI, ExpressJS,NestJS,  Springboot\nFrontend Technologies - HTML, CSS, React, Next, Tailwind, Shadcn \nDatabase -  MongoDB, Postgress, MySql\nTools: VSCode, Postman, Docker, AWS, Git, IntelliJ\nProgramming languages - Python, Javascript, Java\n\nProject 1:  Viz AI\n\nSkills: \nDjango\nReactJS\nAuthentication and Authorization\nPostgres, Multi-tenancy\nLLM: Gemini 2.0 Flash\nPostman\nDescription of the Project: Developed an AI-driven data visualization platform that enables users to seamlessly connect their databases, retrieve schema information, and generate SQL queries through an LLM-powered engine. The generated queries are executed on the connected SQL database, and the results are dynamically visualized using interactive charts. The system empowers non-technical users to gain insights from their data effortlessly, eliminating the need for manual query writing.\nResponsibility: \nDesigned and developed the entire backend using Django, ensuring efficient database interactions and secure API endpoints.\nImplemented authentication and Role-Based Access Control (RBAC) for secure user access.\nImplemented a multi-tenant architecture to ensure data isolation between different users.\nAssisted in the deployment of the web application, optimizing performance and ensuring a seamless user experience.\n\nProject 2: ComplyQuick \u2013 AI-Powered Compliance Training Platform\n\nSkills: \nDjango, ReactJS\nJWT, Bcrypt, Postgres\nRedis, Kafka\nAWS\nPostman\nDescription of the Project: Developed a SaaS-based compliance training platform that enables companies to provide internal training to employees using AI-generated content. The system leverages LLM-based Retrieval-Augmented Generation (RAG) to dynamically generate personalized training materials and multiple-choice questions (MCQs) based on company-provided context. Implemented a multi-tenant architecture to ensure data isolation across organizations. The platform also includes certification issuance, automated training reminders, and analytics features for tracking employee progress and compliance.\nResponsibility: \nDesigned and developed the backend using Django, ensuring efficient API performance and secure data handling.\nDeveloped a multi-tenant architecture to support multiple organizations while maintaining data isolation.\nIntegrated Kafka for real-time event-driven communication across services.\nIntegrated automated training reminders through email and in-app notifications.\nImplemented role-based access control (RBAC) for secure user management.\nAssisted in deploying the platform to AWS, optimizing scalability and reliability.\nProject 3: LexiLearn\n\nSkills: \nNextJS\nDjango\nMongoDb using Djongo\nRedis\nLLM - Gemini 2.0 flash\nDescription of the Project: LexiLearn is an AI-driven language learning platform focused on helping users improve their English proficiency. The system begins with a diagnostic test to assess the user\u2019s skill level. Based on the results, the AI suggests personalized learning goals, which users can modify or add custom goals. The platform generates structured learning tracks that users can study, with AI-powered voiceovers for content delivery. The learning process is adaptive\u2014if users struggle with a topic, the AI slows down and provides in-depth explanations. Users can retest their proficiency, refresh suggested goals, and continuously improve their skills.\nResponsibility: \nDeveloped the entire backend using Django, handling proficiency assessment, goal management, and learning track generation.\nImplemented AI-driven adaptive learning that modifies content delivery based on user comprehension.\nDesigned a system for users to retake diagnostic tests and update their proficiency levels dynamically.\nIntegrated Redis caching to improve response times and system performance.\nAssisted in frontend integration for seamless user interaction and learning experience.\nAssisted in the deployment of the web application, optimizing performance and ensuring a seamless user experience.\n\n\n\n\nProject 4: Truecaller API \n\nSkills: \nDjango\nMongodb using djongo\nJWT and Bcrypt\nPostman\nDescription of the Project: This project enables developers to integrate spam detection, check the name associated with phone number. It also allows the user to check the email attached with the phone number if the user is in the contact list of the phone number. \nResponsibility: \nConceptualized, designed, and developed the API architecture to support caller identification, spam detection, and number verification functionality.\nEnabled seamless integration for third-party applications by creating robust and scalable endpoints\nAuthored comprehensive API documentation, including usage guidelines, authentication methods, and troubleshooting steps, for developers and stakeholders..\n\n\nProject 5: Job Portal\n\nSkills: \nReact (Frontend)\nDjango (Backend) \nPostgreSQL\nShadcn (UI)\nClerk (Authentication & RBAC)\nDescription of the Project: A modern and responsive job portal designed to connect job seekers with employers. The platform provides a seamless experience by integrating Clerk for secure authentication, Shadcn for elegant UI components, and PostgreSQL as the database. The system enables employers to post jobs, while job seekers can apply, manage applications, and receive updates through an intuitive interface.\nResponsibility: \nDeveloped both the frontend and backend as a solo developer, ensuring a full-stack implementation.\nDesigned and implemented a secure authentication system using Clerk with Role-Based Access Control (RBAC).\nBuilt RESTful APIs using Django to manage job postings, applications, and user profiles efficiently.\nIntegrated Shadcn for a modern, responsive, and accessible UI design.\nConfigured and optimized PostgreSQL for storing job listings, user data, and application records.\n\n\n"
enchanced_text = "Name: Sourabh C\nBackend Developer\nWith 2.5 years of experience, I am a driven and adaptable software developer with a passion for programming and tackling challenges. I bring strong analytical and problem-solving skills, a love for learning, and a commitment to delivering impactful solutions. Known for my strong work ethic and continuous growth mindset, I strive to exceed expectations through high-quality, efficient development. My expertise aligns perfectly with the job description for a Python Developer, as I have extensive experience in Django and Django REST Framework (DRF), as well as proficiency in Python and SQL databases (PostgreSQL, MySQL). My projects, including Viz AI and ComplyQuick, have allowed me to work with third-party APIs, webhooks, and system integrations, leveraging tools like Redis and Celery for asynchronous task queues. I have hands-on experience with authentication mechanisms (JWT, OAuth, SSO), and my background in both frontend (ReactJS) and backend development (Django) makes me a versatile developer. Additionally, I have worked extensively on building scalable and efficient platforms, including implementing multi-tenant architecture and role-based access control (RBAC). I am adept at working independently or within a team, and I possess excellent communication skills, which will allow me to seamlessly integrate into your team and contribute effectively. Key Technologies:\nBackend Technologies - Django, FastAPI, ExpressJS, NestJS, Springboot\nFrontend Technologies - HTML, CSS, React, Next, Tailwind, Shadcn\nDatabase - MongoDB, Postgress, MySql\nTools: VSCode, Postman, Docker, AWS, Git, IntelliJ\nProgramming languages - Python, Javascript, Java\nProject 1: Viz AI\nSkills: Django, ReactJS, Authentication and Authorization, Postgres, Multi-tenancy, LLM: Gemini 2.0 Flash, Postman\nDescription of the Project: Developed an AI-driven data visualization platform that enables users to seamlessly connect their databases, retrieve schema information, and generate SQL queries through an LLM-powered engine. The generated queries are executed on the connected SQL database, and the results are dynamically visualized using interactive charts. The system empowers non-technical users to gain insights from their data effortlessly, eliminating the need for manual query writing.\nResponsibility: Designed and developed the entire backend using Django, ensuring efficient database interactions and secure API endpoints. Implemented authentication and Role-Based Access Control (RBAC) for secure user access. Implemented a multi-tenant architecture to ensure data isolation between different users. Assisted in the deployment of the web application, optimizing performance and ensuring a seamless user experience.\nProject 2: ComplyQuick – AI-Powered Compliance Training Platform\nSkills: Django, ReactJS, JWT, Bcrypt, Postgres, Redis, Kafka, AWS, Postman\nDescription of the Project: Developed a SaaS-based compliance training platform that enables companies to provide internal training to employees using AI-generated content. The system leverages LLM-based Retrieval-Augmented Generation (RAG) to dynamically generate personalized training materials and multiple-choice questions (MCQs) based on company-provided context. Implemented a multi-tenant architecture to ensure data isolation across organizations. The platform also includes certification issuance, automated training reminders, and analytics features for tracking employee progress and compliance.\nResponsibility: Designed and developed the backend using Django, ensuring efficient API performance and secure data handling. Developed a multi-tenant architecture to support multiple organizations while maintaining data isolation. Integrated Kafka for real-time event-driven communication across services. Integrated automated training reminders through email and in-app notifications. Implemented role-based access control (RBAC) for secure user management. Assisted in deploying the platform to AWS, optimizing scalability and reliability.\nProject 3: LexiLearn\nSkills: NextJS, Django, MongoDb using Djongo, Redis, LLM - Gemini 2.0 flash\nDescription of the Project: LexiLearn is an AI-driven language learning platform focused on helping users improve their English proficiency. The system begins with a diagnostic test to assess the user’s skill level. Based on the results, the AI suggests personalized learning goals, which users can modify or add custom goals. The platform generates structured learning tracks that users can study, with AI-powered voiceovers for content delivery. The learning process is adaptive—if users struggle with a topic, the AI slows down and provides in-depth explanations. Users can retest their proficiency, refresh suggested goals, and continuously improve their skills.\nResponsibility: Developed the entire backend using Django, handling proficiency assessment, goal management, and learning track generation. Implemented AI-driven adaptive learning that modifies content delivery based on user comprehension. Designed a system for users to retake diagnostic tests and update their proficiency levels dynamically. Integrated Redis caching to improve response times and system performance. Assisted in frontend integration for seamless user interaction and learning experience. Assisted in the deployment of the web application, optimizing performance and ensuring a seamless user experience.\nProject 4: Truecaller API\nSkills: Django, Mongodb using djongo, JWT and Bcrypt, Postman\nDescription of the Project: This project enables developers to integrate spam detection, check the name associated with phone number. It also allows the user to check the email attached with the phone number if the user is in the contact list of the phone number.\nResponsibility: Conceptualized, designed, and developed the API architecture to support caller identification, spam detection, and number verification functionality. Enabled seamless integration for third-party applications by creating robust and scalable endpoints. Authored comprehensive API documentation, including usage guidelines, authentication methods, and troubleshooting steps, for developers and stakeholders.\nProject 5: Job Portal\nSkills: React (Frontend), Django (Backend), PostgreSQL, Shadcn (UI), Clerk (Authentication & RBAC)\nDescription of the Project: A modern and responsive job portal designed to connect job seekers with employers. The platform provides a seamless experience by integrating Clerk for secure authentication, Shadcn for elegant UI components, and PostgreSQL as the database. The system enables employers to post jobs, while job seekers can apply, manage applications, and receive updates through an intuitive interface.\nResponsibility: Developed both the frontend and backend as a solo developer, ensuring a full-stack implementation. Designed and implemented a secure authentication system using Clerk with Role-Based Access Control (RBAC). Built RESTful APIs using Django to manage job postings, applications, and user profiles efficiently. Integrated Shadcn for a modern, responsive, and accessible UI design. Configured and optimized PostgreSQL for storing job listings, user data, and application records.\n"

def extract_hotwords(text):
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    words = clean_text.split()
    stop_words = set(stopwords.words('english')) 
    hotwords = [word for word in words if word.lower() not in stop_words]

    return hotwords


def calculate_similarity(profile_content, jd_content):
    model = SentenceTransformer('all-MiniLM-L6-v2') 

    # Extract hotwords from both texts
    profile_hotwords = extract_hotwords(profile_content)
    jd_hotwords = extract_hotwords(jd_content)
    profile_text = " ".join(profile_hotwords)
    jd_text = " ".join(jd_hotwords)

    embeddings = model.encode([profile_text, jd_text], convert_to_tensor=True)
    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()
    match_percentage = round(similarity_score * 100, 2)
    
    return match_percentage


# print(calculate_similarity(profile_text, enchanced_text))
